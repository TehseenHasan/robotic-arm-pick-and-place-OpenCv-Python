import numpy as np
import cv2 as cv
import time
import serial
import sys
import threading
import yaml
import warnings
warnings.filterwarnings("ignore")

 
#Constants Declaration
webcam_Resolution_Width	= 640.0 #Change the resolution according to your Camera's resolution
webcam_Resolution_Height = 480.0
home_on_boot = True # set True to Home the robotic arm on boot

#Global variables
Gripper_X = np.array([0.0])    #holds X value of Gripper coordinates (1D Numpy Array for float variable)
Gripper_Y = np.array([0.0])    #holds Y value of Gripper coordinates
Object_X  = np.array([0.0])     #holds X value of Object coordinates
Object_Y  = np.array([0.0])     #holds Y value of Object coordinates

#Load Robot position (center, rotation angle etc ) values from file generated by "robot_position_estimation.py" file
with open(r'robot_position.yaml') as file:
    documents = yaml.full_load(file)    #loading yaml file as Stream
    robot_position = np.array(documents['robot_position'])    #extracting robot_position key and convert it into Numpy Array (2D Matrix) 
    robot_x , robot_y , robot_angle , number_of_cm_in_Resolution_width  = robot_position # Robot centroid in pixels and angle with respect to Vertical Axis of Camera frame
    cm_per_pixel = number_of_cm_in_Resolution_width/webcam_Resolution_Width #Convert pixels to cm - tells that how many centimeters are in 1 pixel
    # print ("\nRobot Position\n",robot_position)
    print("\Robot Position Matrix Loaded Succeccfully\n")
    
#Homogeneous transformation matrix for Coordinates transformation (camera to robotic arm)
#Rotation Vector (Visit http://www.it.hiof.no/~borres/j3d/math/homo/p-homo.html for references)
#as out robot coordinate has to be only rotated along Y axis : +180 degrees
R0_C = [[np.cos(np.pi),0,np.sin(np.pi)],[0,1,0],[-np.sin(np.pi),0,np.cos(np.pi)]]
#Displacement Vector - Find using "robot_position_estimation.py" file
robot_distance_from_X = robot_x # cm - distance from Image origin X axis to the robotic arm origin
robot_distance_from_Y = -robot_y # cm - distance from Image origin Y axis to the robotic arm origin
d0_C = [[robot_distance_from_X],[robot_distance_from_Y],[0]]  #distance between Robot origin to the Camera coordinates orign
#Homogeneous transformation matrix
H0_C = np.concatenate((R0_C,d0_C),1)
H0_C = np.concatenate((H0_C,[[0,0,0,1]]),0)



#Connecting with Arduino - Robotic Arm    
try:
    arduino = serial.Serial('COM8', 115200, timeout=.1)
    time.sleep(1) #give the connection a second to settle
    if(home_on_boot):
        arduino.write(str("G28\r\n").encode()) #send Homming command to Robotic Arm
        time.sleep(16)  #wait while Homming procedure completes
        arduino.write(str("M5\r\n").encode()) #Close the gripper
    arduino.flushInput() 
    time.sleep(2)
except:
    print("Arduino communication ERROR")
    sys.exit()



def ArduinoCommands():  # Function to Send Pick and Place Commands to Arduino-Robotic Arm
    try:
        #Move the gripper to the position of the object and pick it then place it on another place
        #Pick
        command_to_Arduino = str("G0X"+str(Object_X[0])+"Y"+str(Object_Y[0])+"F80\r\n")
        print(command_to_Arduino)
        arduino.write(command_to_Arduino.encode())  #move to the object
        arduino.write(str("M3\r\n").encode()) #Open the gripper
        time.sleep(0.5)
        arduino.write(str("M3\r\n").encode())
        time.sleep(1)
        arduino.write(str("G0Z-110F80\r\n").encode()) # lower the gripper to pick the object
        arduino.write(str("M5\r\n").encode()) #Close the gripper - Object picked
        arduino.write(str("M5\r\n").encode())
        arduino.write(str("G0Z0F80\r\n").encode()) # higher the gripper after picking the object
        time.sleep(1)
        
        #Place
        arduino.write(str("G0 X-210 Y120 Z0 F80\r\n").encode())  #move to the place location
        time.sleep(2)
        arduino.write(str("G0Z-110F80\r\n").encode()) # lower the gripper to place the object
        time.sleep(2)
        arduino.write(str("M3\r\n").encode()) #Open the gripper
        time.sleep(3)
        arduino.write(str("M3\r\n").encode()) #Open the gripper
        time.sleep(2)
        arduino.write(str("G0Z0F80\r\n").encode())
        time.sleep(2)
        arduino.write(str("G0 X0 Y170 Z120 F80\r\n").encode()) # go to the parking position
        time.sleep(2)
        arduino.write(str("M5\r\n").encode()) #Close the gripper
        time.sleep(2)
        arduino.write(str("M5\r\n").encode()) #Close the gripper    
        time.sleep(3)
    except Exception as e:
        print(e)


def LiveLocationOfGripper():    #Function to get Live coordinates of the Gripper of Robotic Arm
    cap = cv.VideoCapture(0, cv.CAP_DSHOW)    
    while getattr(t0, "do_run", True):
        try:
            _,frame = cap.read()    #reading singe frame from camera

            #Track the Gripper of Robotic Arm using Color Subtraction (RED colored)
            red = np.matrix(frame[:,:,2])  #extracting red layer (layer No 2) from RGB
            green = np.matrix(frame[:,:,1]) #extracting green layer (layer No 1) from RGB
            blue = np.matrix(frame[:,:,0])  #extracting blue layer (layer No 0) from RGB    
            #it will display only the red colored objects bright with black background
            red_only = np.int16(red)-np.int16(green)-np.int16(blue)
            
            # converting Grayscale image to Black&White (applying thresholding)
            # _,red_only = cv.threshold(red_only,70,200,cv.THRESH_BINARY)            
            red_only[red_only<50] =0
            red_only[red_only>255] =255
            
            red_only = np.uint8(red_only)
            
            # cv.namedWindow('Live Robot-Gripper', cv.WINDOW_AUTOSIZE)
            # cv.imshow('Live Robot-Gripper',red_only)
            # cv.waitKey(1)
            
            #Find Location of Robot Gripper in cm units
            # X location (in pixels)
            column_sums = np.matrix(np.sum(red_only,0))
            column_numbers = np.matrix(np.arange(webcam_Resolution_Width))
            column_mult = np.multiply(column_sums,column_numbers)
            total = np.sum(column_mult)
            total_total = np.sum(np.sum(red_only))  #sum of all the pixels
            column_location = total/total_total # the location of the column where the center of mass exists        
            # Y location (in pixels)
            row_sums = np.matrix(np.sum(red_only,1))
            row_sums = row_sums.transpose()
            row_numbers = np.matrix(np.arange(webcam_Resolution_Height))
            row_mult = np.multiply(row_sums,row_numbers)
            total = np.sum(row_mult)
            total_total = np.sum(np.sum(red_only))  #sum of all the pixels
            row_location = total/total_total # the location of the column where the center of mass exists    
            #Location on Gripper in Camera coordinates (pixels)
            #print(column_location,row_location)
            
            if np.isnan(column_location) or np.isnan(row_location):
                    # print("Gripper NOT Found!")
                    continue
            
            #Display coordinates, text, etc. on detected gripper location
            #Display Circle
            red_only = cv.circle(red_only, (int(column_location), int(row_location)), 40, (255,255,255), 2) #image, center coordinates (x, y), radius of the circle, stroke color in BGR, stroke thickness  (in pixels)
            #Display center point
            red_only = cv.circle(red_only, (int(column_location), int(row_location)), 2, (255,255,255), 3) #image, center coordinates (x, y), radius of the circle, stroke color in BGR, stroke thickness  (in pixels)
            
            
            #Location on Gripper in World coordinates  (centimeters)
            x_location1 = column_location * cm_per_pixel
            y_location1 = row_location * cm_per_pixel
            # print(x_location1,y_location1)    
            #Gripper in camera axis
            PC = [[x_location1],[y_location1],[0],[1]]    
            #Gripper in robot axis (using homogeneous transformation matrix)
            P0 = np.dot(H0_C,PC)
            
            #These are global variables - I am using these variables in main loop below as process value in PID algorithm
            #Location of Gripper in Robot coordinates
            Gripper_X[0] = P0[0]  #x coordinate of gripper in cm
            Gripper_Y[0] = P0[1]  #y coordinate of gripper in cm
                                       
            #convert cm to mm and Round off to 0 decimal point
            Gripper_X[0] = round(Gripper_X[0]*10, 0)
            Gripper_Y[0] = round(Gripper_Y[0]*10, 0)
            # print("Live Gripper Location: ")
            # print(Gripper_X[0],Gripper_Y[0])
            time.sleep(0.1)
            
            #Draw coordinates on image (location of object in mm in Robot coordinates)
            #cv2.putText(img, text, position, font, fontScale, color, thickness, lineType, bottomLeftOrigin)
            cv.putText(red_only, str(str(Gripper_X[0])+" , "+str(Gripper_Y[0])), (int(column_location), int(row_location)-30), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv.LINE_AA)         
            cv.namedWindow('Live Robot-Gripper', cv.WINDOW_AUTOSIZE)
            cv.imshow('Live Robot-Gripper',red_only)
            cv.waitKey(1)

        except Exception as e:
            print(e)
            cv.destroyWindow("Live Robot-Gripper")
            pass

def calculate_XYZ(u,v):    #Function to get World Coordinates from Camera Coordinates
        cam_mtx = camera_matrix
        Rt = extrinsic_matrix
        #Solve: From Image Pixels, find World Points

        scalingfactor = 40.0 #this is demo value, Calculate the Scaling Factor first
        tvec1 = Rt[:, 3]  #Extract the 4th Column (Translation Vector) from Extrinsic Matric
        
        
        uv_1=np.array([[u,v,1]], dtype=np.float32)
        uv_1=uv_1.T
        suv_1=scalingfactor*uv_1
        inverse_cam_mtx = np.linalg.inv(cam_mtx)
        xyz_c=inverse_cam_mtx.dot(suv_1)
        xyz_c=xyz_c-tvec1
        R_mtx = Rt[:,[0,1,2]] #Extract first 3 columns (Rotation Matrix) from Extrinsics Matrix
        inverse_R_mtx = np.linalg.inv(R_mtx)
        XYZ=inverse_R_mtx.dot(xyz_c)
        return XYZ




# print("\nTesting calculate_XYZ function with demo values\n")
# print(calculate_XYZ(600,400))


if __name__ == "__main__":
    try:
        cap = cv.VideoCapture(0, cv.CAP_DSHOW)    # reading camera feed (https://answers.opencv.org/question/227535/solvedassertion-error-in-video-capturing/))
        # Capture image of background without any objects    
        time.sleep(2)
        _,frame = cap.read()    #reading singe frame from camera
        print("Background Image taken and Undistorted Successfully")
        #convert RGB to Grayscale
        gray_image1 = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)
        cv.namedWindow('background', cv.WINDOW_AUTOSIZE)
        cv.imshow("background",gray_image1)
        cv.waitKey(1)
    except:
        print("Camera Connection ERROR")
        arduino.close()
        sys.exit()


    while(1):
        try:
            
            # Start a thread to get live location of Robotic Arm gripper
            try:
                t0 = threading.Thread(target=LiveLocationOfGripper)    # https://www.geeksforgeeks.org/multithreading-python-set-1/
                t0.start()   # starting thread 
                #t0.join()    # wait until thread is completely executed 
            except:
                print ("Error: unable to start thread")
                
            #Now Place the target object on the surface and then press Enter key to proceed further.
            while(1):
                _,temp = cap.read()
                cv.imshow("Live" , temp)
                k = cv.waitKey(5)
                
                if k == 13: #Move to next code by pressing Enter key
                    #Capturing single frame (image) from camera
                    _,frame = cap.read()
                    #frame = undistortImage(frame)
                    print("Foreground Image taken and Undistorted Successfully")
                    break
                if k == 27: #exit by pressing Esc key
                    cv.destroyAllWindows()
                    arduino.close()
                    t0.do_run = False # Stop Thread0 (https://stackoverflow.com/a/36499538/3661547)
                    sys.exit()
                if k == 104: #Home the robotic arm by pressing h key
                    arduino.write(str("M17\r\n").encode())  # Turn ON Stepper
                    arduino.write(str("G28\r\n").encode()) #send Homming command to Robotic Arm
                    time.sleep(15)
                if k == 120: #Turn OFF Steppers of the robotic arm by pressing x key
                    arduino.write(str("M18\r\n").encode()) #send command to Robotic Arm
                    time.sleep(1)
                if k == 117: #Update background by pressing u key
                    _,frame = cap.read()    #reading singe frame from camera
                    #frame = undistortImage(frame)
                    print("Background Image taken and Undistorted Successfully")
                    #convert RGB to Grayscale
                    gray_image1 = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)
                    cv.namedWindow('background', cv.WINDOW_AUTOSIZE)
                    cv.imshow("background",gray_image1)
                    cv.waitKey(1)
                    
                            
        
            #Now Detect the target object to be picked by Robotic Arm
            #convert RGB to Grayscale
            gray_image2 = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)
            #cv.imshow('foreground',gray_image2)
            #Object detection using Difference method
            difference = np.absolute(np.matrix(np.int16(gray_image1))-np.matrix(np.int16(gray_image2)))
            difference[difference>255] = 255
            difference = np.uint8(difference)
            #cv.imshow('object',difference)
            
            # converting Grayscale image to Black&White (applying thresholding)
            BW = difference
            BW[BW<=100] = 0
            BW[BW>100] = 1
            
            #now we have to find the center of mass of the object (in pixel units)
            # X location (in pixels)
            column_sums = np.matrix(np.sum(BW,0))
            column_numbers = np.matrix(np.arange(webcam_Resolution_Width))
            column_mult = np.multiply(column_sums,column_numbers)
            total = np.sum(column_mult)
            total_total = np.sum(np.sum(BW))  #sum of all the pixels
            column_location = total/total_total # the location of the column where the center of mass exists
            #print(column_location)       
            # Y location (in pixels)
            row_sums = np.matrix(np.sum(BW,1))
            row_sums = row_sums.transpose()
            row_numbers = np.matrix(np.arange(webcam_Resolution_Height))
            row_mult = np.multiply(row_sums,row_numbers)
            total = np.sum(row_mult)
            total_total = np.sum(np.sum(BW))  #sum of all the pixels
            row_location = total/total_total # the location of the column where the center of mass exists    
            #Location on object in Camera coordinates (pixels)
            #print(column_location,row_location)
            
            if np.isnan(column_location) or np.isnan(row_location):
                print("No object Found!")
                continue
            
            #Display coordinates, text, etc. on detected object location
            #Display Circle
            frame = cv.circle(frame, (int(column_location), int(row_location)), 20, (0,0,255), 2) #image, center coordinates (x, y), radius of the circle, stroke color in BGR, stroke thickness  (in pixels)
            #Display center point
            frame = cv.circle(frame, (int(column_location), int(row_location)), 2, (255,0,0), 3) #image, center coordinates (x, y), radius of the circle, stroke color in BGR, stroke thickness  (in pixels)
            
            
            #Location on object in World coordinates  (centimeters)
            x_location2 = column_location * cm_per_pixel
            y_location2 = row_location * cm_per_pixel
            # print(x_location2,y_location2)   
            #Point in camera axis
            PC = [[x_location2],[y_location2],[0],[1]]    
            #Point in robot axis (using homogeneous transformation matrix)
            P0 = np.dot(H0_C,PC)    
            Object_X[0] = P0[0]
            Object_Y[0] = P0[1]
            # print location in cm
            print(Object_X[0],Object_Y[0])
    
            #convert cm to mm then send to rootic arm and also round off the values.
            Object_X[0] = round(Object_X[0]*10 , 0) #Round off to 0 decimal point and convert to mm
            Object_Y[0] = round(Object_Y[0]*10 , 0)
            #Location of object in Robot coordinates in Milimeters
            # print(Object_X[0],Object_Y[0])
            
            #Draw coordinates on image (location of object in mm in Robot coordinates)
            #cv2.putText(img, text, position, font, fontScale, color, thickness, lineType, bottomLeftOrigin)
            cv.putText(frame, str(str(Object_X[0])+" , "+str(Object_Y[0])), (int(column_location), int(row_location)-30), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv.LINE_AA)
            cv.namedWindow('Detected Object', cv.WINDOW_AUTOSIZE)
            cv.imshow('Detected Object',frame)
            cv.waitKey(1)
            
            
            # Send location of object to Robotic Arm for pick and place operation
            # Create a thread to run the Robot Commands
            try:
                t1 = threading.Thread(target=ArduinoCommands)    # https://www.geeksforgeeks.org/multithreading-python-set-1/
                t1.start()   # starting thread 
                #t1.join()    # wait until thread is completely executed 
            except:
                print ("Error: unable to start thread")
            
            
        
        except Exception as e:
            print("Error in Main Loop\n",e)
            cv.destroyAllWindows()
            arduino.close()    
            t0.do_run = False # Stop Thread0 (https://stackoverflow.com/a/36499538/3661547)
            sys.exit()
    
    cv.destroyAllWindows()
    


